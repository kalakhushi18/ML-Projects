{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: LM Studio \n",
    "\n",
    "This files contains the implementation code for LM Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-p8y2vormonjks6x3sduqa', 'object': 'chat.completion', 'created': 1731421238, 'model': 'llama-3.2-1b-instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Large Language Models (LLMs) are a type of artificial intelligence (AI) designed to process and understand human language. They were developed by Google and other companies, and have been widely used in various applications.\\n\\nHere's an overview of how LLMs work:\\n\\n**Key Components:**\\n\\n1. **Encoder-Decoder Architecture:** An encoder extracts features from input text, while a decoder generates output based on those features.\\n2. **Self-Supervised Learning:** The model is trained using self-supervised learning objectives, which involve predicting the next word in a sequence of text or other natural language inputs.\\n3. **Training Data:** Large amounts of text data are used to train the model, which includes various genres, styles, and lengths of texts.\\n\\n**How LLMs Learn:**\\n\\n1. **Text Preprocessing:** The input text is preprocessed to remove punctuation, convert it to lowercase, and normalize the text.\\n2. **Embeddings:** The preprocessed text is then converted into numerical embeddings using techniques such as Word2Vec or GloVe.\\n3. **Encoder:** An encoder layer is applied to the embedded inputs, which extracts features from the input sequence of words.\\n4. **Decoder:** A decoder layer is then used to generate output based on those features.\\n5. **Loss Function:** The model is trained using a loss function such as cross-entropy or masked language modeling, which measures the difference between predicted and actual outputs.\\n\\n**Advantages:**\\n\\n1. **Contextual Understanding:** LLMs have been shown to understand context better than other AI models, including humans.\\n2. **Language Generation:** They can generate human-like text, including articles, stories, and even entire books.\\n3. **Translation:** LLMs are highly effective for machine translation tasks.\\n\\n**Applications:**\\n\\n1. **Chatbots:** LLMs are used in chatbot platforms to engage with users, answer questions, and provide customer support.\\n2. **Content Generation:** They can generate content such as articles, blog posts, social media updates, and even entire websites.\\n3. **Speech Recognition:** LLMs can be integrated into speech recognition systems for voice-to-text applications.\\n\\n**Challenges:**\\n\\n1. **Overfitting:** LLMs can suffer from overfitting if the training data is not diverse enough or if the model is too complex.\\n2. **Adversarial Examples:** They can be vulnerable to adversarial examples, which are designed to mislead the model into producing incorrect outputs.\\n\\n**Current Trends:**\\n\\n1. **Multimodal Training:** LLMs are being trained on multimodal data, including text and visual inputs (e.g., images).\\n2. **Explainability:** There is an increasing focus on developing explainable AI models that can provide insights into their decision-making processes.\\n3. **Edge AI:** The development of edge AI applications that require real-time processing and low latency are becoming increasingly important.\\n\\nOverall, Large Language Models have the potential to revolutionize various industries, including technology, customer service, content creation, and more. However, they also raise complex questions about fairness, transparency, and explainability in AI decision-making processes.\"}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 16, 'completion_tokens': 642, 'total_tokens': 658}, 'system_fingerprint': 'llama-3.2-1b-instruct'}\n",
      "The response is: Large Language Models (LLMs) are a type of artificial intelligence (AI) designed to process and understand human language. They were developed by Google and other companies, and have been widely used in various applications.\n",
      "\n",
      "Here's an overview of how LLMs work:\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "1. **Encoder-Decoder Architecture:** An encoder extracts features from input text, while a decoder generates output based on those features.\n",
      "2. **Self-Supervised Learning:** The model is trained using self-supervised learning objectives, which involve predicting the next word in a sequence of text or other natural language inputs.\n",
      "3. **Training Data:** Large amounts of text data are used to train the model, which includes various genres, styles, and lengths of texts.\n",
      "\n",
      "**How LLMs Learn:**\n",
      "\n",
      "1. **Text Preprocessing:** The input text is preprocessed to remove punctuation, convert it to lowercase, and normalize the text.\n",
      "2. **Embeddings:** The preprocessed text is then converted into numerical embeddings using techniques such as Word2Vec or GloVe.\n",
      "3. **Encoder:** An encoder layer is applied to the embedded inputs, which extracts features from the input sequence of words.\n",
      "4. **Decoder:** A decoder layer is then used to generate output based on those features.\n",
      "5. **Loss Function:** The model is trained using a loss function such as cross-entropy or masked language modeling, which measures the difference between predicted and actual outputs.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "1. **Contextual Understanding:** LLMs have been shown to understand context better than other AI models, including humans.\n",
      "2. **Language Generation:** They can generate human-like text, including articles, stories, and even entire books.\n",
      "3. **Translation:** LLMs are highly effective for machine translation tasks.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "1. **Chatbots:** LLMs are used in chatbot platforms to engage with users, answer questions, and provide customer support.\n",
      "2. **Content Generation:** They can generate content such as articles, blog posts, social media updates, and even entire websites.\n",
      "3. **Speech Recognition:** LLMs can be integrated into speech recognition systems for voice-to-text applications.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "1. **Overfitting:** LLMs can suffer from overfitting if the training data is not diverse enough or if the model is too complex.\n",
      "2. **Adversarial Examples:** They can be vulnerable to adversarial examples, which are designed to mislead the model into producing incorrect outputs.\n",
      "\n",
      "**Current Trends:**\n",
      "\n",
      "1. **Multimodal Training:** LLMs are being trained on multimodal data, including text and visual inputs (e.g., images).\n",
      "2. **Explainability:** There is an increasing focus on developing explainable AI models that can provide insights into their decision-making processes.\n",
      "3. **Edge AI:** The development of edge AI applications that require real-time processing and low latency are becoming increasingly important.\n",
      "\n",
      "Overall, Large Language Models have the potential to revolutionize various industries, including technology, customer service, content creation, and more. However, they also raise complex questions about fairness, transparency, and explainability in AI decision-making processes.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "local_api_url = \"http://localhost:1234/v1/chat/completions\"  \n",
    "role = 'user'\n",
    "content = \"Give the explanation for LLM\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"llama-3.2-1b-instruct\",  \n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(local_api_url, json=payload)\n",
    "json_response = response.json()\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "   \n",
    "    print(response.json())\n",
    "    print(\"The response is:\", json_response['choices'][0]['message']['content'])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-vbfxri0m798ihocufbat7', 'object': 'chat.completion', 'created': 1731422303, 'model': 'llama-3.2-1b-instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Hier ist der √úbersetzung:\\n\\nEin Desktop-Application f√ºr lokale LLMs-Modelle\\nEin vertrautes Konferenzschaltbild\\nSuch- und Download-Funktionen (via Hugging Face) ü§ó\\nEine lokale Server-Instanz, die auf OpenAI√§hnliche Endpunkte anh√∂rt\\nSysteme f√ºr das Verwalten lokaler Modelle und Konfigurationen\\n\\nIch habe einige Dinge ge√§ndert:\\n\\n* \"A familiar\" -> \"Ein vertrautes\"\\n* \"Search & download functionality\" -> \"Such- und Download-Funktionen\"\\n* \"Hugging Face\" -> \"Hugging Face\" (durch Kehrw√ºrfe der urspr√ºnglichen Wortfolge)\\n\\nDie √úbersetzung ist insbesondere in den letzten beiden S√§tzen nicht vollst√§ndig, da sie im Original nur die erste Zeile enth√§lt. Hier ist eine korrigierte Version:\\n\\nEin Desktop-Application f√ºr lokale LLMs-Modelle\\nEin vertrautes Konferenzschaltbild\\nSuch- und Download-Funktionen (via Hugging Face) ü§ó\\nEine lokale Server-Instanz, die auf OpenAI√§hnliche Endpunkte anh√∂rt\\nSysteme zum Verwalten lokaler Modelle und Konfigurationen\\n\\nIch hoffe, das hilft!'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 68, 'completion_tokens': 280, 'total_tokens': 348}, 'system_fingerprint': 'llama-3.2-1b-instruct'}\n",
      "The response is: Hier ist der √úbersetzung:\n",
      "\n",
      "Ein Desktop-Application f√ºr lokale LLMs-Modelle\n",
      "Ein vertrautes Konferenzschaltbild\n",
      "Such- und Download-Funktionen (via Hugging Face) ü§ó\n",
      "Eine lokale Server-Instanz, die auf OpenAI√§hnliche Endpunkte anh√∂rt\n",
      "Systeme f√ºr das Verwalten lokaler Modelle und Konfigurationen\n",
      "\n",
      "Ich habe einige Dinge ge√§ndert:\n",
      "\n",
      "* \"A familiar\" -> \"Ein vertrautes\"\n",
      "* \"Search & download functionality\" -> \"Such- und Download-Funktionen\"\n",
      "* \"Hugging Face\" -> \"Hugging Face\" (durch Kehrw√ºrfe der urspr√ºnglichen Wortfolge)\n",
      "\n",
      "Die √úbersetzung ist insbesondere in den letzten beiden S√§tzen nicht vollst√§ndig, da sie im Original nur die erste Zeile enth√§lt. Hier ist eine korrigierte Version:\n",
      "\n",
      "Ein Desktop-Application f√ºr lokale LLMs-Modelle\n",
      "Ein vertrautes Konferenzschaltbild\n",
      "Such- und Download-Funktionen (via Hugging Face) ü§ó\n",
      "Eine lokale Server-Instanz, die auf OpenAI√§hnliche Endpunkte anh√∂rt\n",
      "Systeme zum Verwalten lokaler Modelle und Konfigurationen\n",
      "\n",
      "Ich hoffe, das hilft!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "role = 'user'\n",
    "content = \"\"\"Can you transalte the following text in German Language:\n",
    "\n",
    "A desktop application for running local LLMs\n",
    "A familiar chat interface\n",
    "Search & download functionality (via Hugging Face ü§ó)\n",
    "A local server that can listen on OpenAI-like endpoints\n",
    "Systems for managing local models and configurations\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"llama-3.2-1b-instruct\",  \n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(local_api_url, json=payload)\n",
    "json_response = response.json()\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "   \n",
    "    print(response.json())\n",
    "    print(\"The response is:\", json_response['choices'][0]['message']['content'])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "\n",
    "Lm Studio has a very easy interface to easily access large Language Models and implement them as OpenAI Like APIs in the python code. Model can be loaded directly into the LM Studio and when the server is turned on it can be connected locally with the python file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
