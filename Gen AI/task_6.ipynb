{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Multi Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "llm = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cards:\n",
      "Here are some creative and informative card options:\n",
      "\n",
      "• **Is your LLM-powered product generating high-quality responses?**\n",
      "\t+ Check out our new tool, LLM-as-a-Judge, to assess its quality\n",
      "• **Get a second opinion on your chatbot or Q&A system's output!**\n",
      "\t+ Use LLM-as-a-Judge to evaluate the effectiveness of AI-powered products\n",
      "• **Boost your conversational AI with this innovative evaluation method!**\n",
      "\t+ Learn how LLM-as-a-Judge can help you improve your product's performance\n",
      "• **Take a closer look at your chatbot's language quality with our new tool!**\n",
      "\t+ Discover if your chatbot is producing high-quality responses and learn how to fix them\n",
      "• **Elevate your Q&A system's accuracy with this expert evaluation method!**\n",
      "\t+ Use LLM-as-a-Judge to assess the effectiveness of your AI-powered Q&A product\n"
     ]
    }
   ],
   "source": [
    "# Define the text input\n",
    "input_text = \"\"\"\n",
    "LLM-as-a-Judge is an evaluation method to assess the quality of text outputs from any LLM-powered product, including chatbots, Q&A systems, or agents. It uses a large language model (LLM) with an evaluation prompt to rate generated text based on criteria you define.\n",
    "\"\"\"\n",
    "\n",
    "# Generator prompt\n",
    "generator_prompt = f\"\"\"\n",
    "You are a generator agent. Your task is to generate creative and informative cards from the following text:\n",
    "\"{input_text}\"\n",
    "Output only the generated cards in bullet point format.\n",
    "\"\"\"\n",
    "\n",
    "# Generate cards\n",
    "generator_response = llm.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": generator_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.2-1b-instruct\",\n",
    ")\n",
    "\n",
    "print(\"Generated Cards:\")\n",
    "generator_response = generator_response.choices[0].message.content\n",
    "print(generator_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM's Opinion on Agents Without Tools:\n",
      "This is a classic example of the \"agent\" vs. \"actor\" distinction in artificial intelligence and cognitive science.\n",
      "\n",
      "While the term \"agent\" is often associated with intelligent systems that can perform tasks autonomously, it's indeed possible to define agents without relying on tools or actions. In this case, we would need to consider the agent's ability to generate text as a fundamental aspect of its behavior.\n",
      "\n",
      "In this scenario, I would argue that yes, an agent can still qualify if:\n",
      "\n",
      "1. The generator relies solely on generating text and does not use any external tools or interactions.\n",
      "2. The generation of text is the primary purpose and functionality of the system.\n",
      "3. The text generation process involves cognitive processes such as pattern recognition, inference, and decision-making.\n",
      "\n",
      "In other words, an agent can be defined without relying on tools if it's primarily a text generator that uses its internal processing capabilities to produce text.\n",
      "\n",
      "However, there are some subtleties to consider:\n",
      "\n",
      "* If the system relies on external inputs or data (e.g., databases, APIs), it may still be considered an actor, as these inputs influence its behavior.\n",
      "* If the agent is designed for other purposes beyond text generation (e.g., natural language processing, question answering, summarization), it's more likely to be classified as an actor.\n",
      "\n",
      "So, while a non-tool-using generator can be thought of as an agent in certain contexts, it's not necessarily so without further context and definitions.\n"
     ]
    }
   ],
   "source": [
    "discussion_prompt = \"\"\"\n",
    "An agent typically involves tools or actions it can perform. If a generator does not use any tools and only relies on generating text, does it still qualify as an agent? Explain your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "discussion_response = llm.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": discussion_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.2-1b-instruct\",\n",
    ")\n",
    "print(\"LLM's Opinion on Agents Without Tools:\")\n",
    "discussion_response = discussion_response.choices[0].message.content\n",
    "print(discussion_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewer Feedback:\n",
      "**Card Evaluation Report: Chat Completion**\n",
      "\n",
      "Based on the provided card options, here's my evaluation and suggested improvements:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* The card offers a clear call-to-action (CTA) with multiple specific suggestions.\n",
      "* The use of conversational tone and emojis is engaging.\n",
      "\n",
      "**Weaknesses and Areas for Improvement:**\n",
      "\n",
      "1. **Lack of context:** The CTA seems out of place in the provided code snippet. It's not immediately clear what this card is intended to accomplish or how it interacts with other cards in a conversation.\n",
      "2. **Insufficient descriptive text:** While the choices are creative and informative, they lack descriptive text that would help users understand the purpose and benefits of each option.\n",
      "3. **No clear structure:** The CTA appears to be a standalone suggestion without any connection to other card options or prompts.\n",
      "\n",
      "**Suggestions:**\n",
      "\n",
      "1. **Rephrase the CTA:** Consider rephrasing the CTA to make it more concise and focused on what users can do with the card, such as \"Get started with LLM-as-a-Judge\" or \"Assess your chatbot's quality.\"\n",
      "2. **Add descriptive text:** Include brief descriptions for each option, highlighting their benefits and relevance to the context of the conversation.\n",
      "3. **Reorganize the structure:** Consider grouping related options together or creating a clear hierarchy to make it easier for users to navigate the card.\n",
      "\n",
      "**Example Reorganization:**\n",
      "\n",
      "Here's an example of how the card could be reorganized:\n",
      "\n",
      "* Card 1: \"Get started with LLM-as-a-Judge\"\n",
      "\t+ Description: \"Assess your chatbot's quality and identify areas for improvement\"\n",
      "\t+ CTA: \"Try LLM-as-a-Judge to evaluate your product\"\n",
      "\n",
      "* Card 2: \"Elevate your Q&A system's accuracy\"\n",
      "\t+ Description: \"Use LLM-as-a-Judge to assess the effectiveness of your AI-powered Q&A product\"\n",
      "\t+ CTA: \"Take a closer look at your chatbot's language quality with this expert evaluation method!\"\n",
      "\n",
      "By addressing these areas for improvement, you can enhance the usability and effectiveness of the card in facilitating more productive conversations.\n"
     ]
    }
   ],
   "source": [
    "# Reviewer prompt\n",
    "reviewer_prompt = f\"\"\"\n",
    "You are a reviewer agent. Your task is to evaluate the following cards and suggest improvements:\n",
    "{generator_response}\n",
    "Your evaluation should identify areas for improvement and provide constructive feedback.\n",
    "\"\"\"\n",
    "\n",
    "# Review the cards\n",
    "reviewer_response = llm.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": reviewer_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.2-1b-instruct\",\n",
    ")\n",
    "print(\"Reviewer Feedback:\")\n",
    "reviewer_response = reviewer_response.choices[0].message.content\n",
    "print(reviewer_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edited Cards:\n",
      "Here is the revised code based on the feedback provided:\n",
      "\n",
      "```\n",
      "ChatCompletion(id='chatcmpl-wvkhq7jkdlx6n7p2q6aj', choices=[\n",
      "  Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Here are some creative and informative card options:\\n\\n• **Is your LLM-powered product generating high-quality responses?**\\n\\t+ Check out our new tool, LLM-as-a-Judge, to assess its quality\\n• **Get a second opinion on your chatbot or Q&A system's output!**\\n\\t+ Use LLM-as-a-Judge to evaluate the effectiveness of AI-powered products\\n\",\n",
      "           reason='evaluation', description='Evaluates the quality of your chatbot or Q&A system',\n",
      "           type='question', role='assistant')\n",
      "], created=1733503344, model='llama-3.2-1b-instruct', object='chat.completion', service_tier=None, system_fingerprint='llama-3.2-1b-instruct', usage=CompletionUsage(completion_tokens=188, prompt_tokens=101, total_tokens=289, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "\n",
      "ChatCompletion(id='chatcmpl-wvkhq7jkdlx6n7p2q6aj', choices=[\n",
      "  Choice(finish_reason='stop', index=1, logprobs=None, message=ChatCompletionMessage(content=\"Here are some creative and informative card options:\\n\\n• **Boost your conversational AI with this innovative evaluation method!**\\n\\t+ Learn how LLM-as-a-Judge can help you improve your product's performance\\n\",\n",
      "           reason='evaluation', description='Assesses the effectiveness of a conversational AI',\n",
      "           type='question', role='assistant')\n",
      "], created=1733503344, model='llama-3.2-1b-instruct', object='chat.completion', service_tier=None, system_fingerprint='llama-3.2-1b-instruct', usage=CompletionUsage(completion_tokens=188, prompt_tokens=101, total_tokens=289, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "\n",
      "ChatCompletion(id='chatcmpl-wvkhq7jkdlx6n7p2q6aj', choices=[\n",
      "  Choice(finish_reason='stop', index=2, logprobs=None, message=ChatCompletionMessage(content=\"Here are some creative and informative card options:\\n\\n• **Take a closer look at your chatbot's language quality with our new tool!**\\n\\t+ Discover if your chatbot is producing high-quality responses and learn how to fix them\",\n",
      "           reason='evaluation', description='Evaluates the language quality of a chatbot',\n",
      "           type='question', role='assistant')\n",
      "], created=1733503344, model='llama-3.2-1b-instruct', object='chat.completion', service_tier=None, system_fingerprint='llama-3.2-1b-instruct', usage=CompletionUsage(completion_tokens=188, prompt_tokens=101, total_tokens=289, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "\n",
      "ChatCompletion(id='chatcmpl-wvkhq7jkdlx6n7p2q6aj', choices=[\n",
      "  Choice(finish_reason='stop', index=3, logprobs=None, message=ChatCompletionMessage(content=\"Here are some creative and informative card options:\\n\\n• **Elevate your Q&A system's accuracy with this expert evaluation method!**\\n\\t+ Use LLM-as-a-Judge to assess the effectiveness of your AI-powered Q&A product\",\n",
      "           reason='evaluation', description='Assesses the effectiveness of a Q&A system',\n",
      "           type='question', role='assistant')\n",
      "], created=1733503344, model='llama-3.2-1b-instruct', object='chat.completion', service_tier=None, system_fingerprint='llama-3.2-1b-instruct', usage=CompletionUsage(completion_tokens=188, prompt_tokens=101, total_tokens=289, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Editor prompt\n",
    "editor_prompt = f\"\"\"\n",
    "You are an editor agent. Your task is to revise the following cards based on the feedback provided:\n",
    "Cards:\n",
    "{generator_response}\n",
    "Feedback:\n",
    "{reviewer_response}\n",
    "Your output should include a revised version of the cards.\n",
    "\"\"\"\n",
    "\n",
    "# Edit the cards\n",
    "\n",
    "\n",
    "editor_response = llm.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": editor_prompt,\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.2-1b-instruct\",\n",
    ")\n",
    "\n",
    "editor_response = editor_response.choices[0].message.content\n",
    "print(\"Edited Cards:\")\n",
    "print(editor_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
